package v3

// Response represents the response from the API.
type Response struct {
	// ID is the unique identifier of the message.
	ID string `json:"id"`
	// Content generated by the model. This is an array of content blocks, each of which has a type
	// that determines its shape. Currently, the only type in responses is "text".
	Content []*MessageContent `json:"content"`
	// Model is the model that performed the completion.
	Model Model `json:"model"`
	// Role is the conversational role of the generated message. This will always be "assistant".
	Role Role `json:"role"`
	// StopReason is the reason that Claude stopped. This may be one the following values:
	//
	// "end_turn": the model reached a natural stopping point.
	// "max_tokens": we exceeded the requested max_tokens or the model's maximum.
	// "stop_sequence": one of your provided custom stop_sequences was generated.
	StopReason string `json:"stop_reason"`
	// StopSequence represents which custom stop sequence was generated, if any.
	// This value will be a non-null string if one of your custom stop sequences was generated.
	StopSequence *string `json:"stop_sequence"`
	// Type is the Object type. For Messages, this is always "message".
	Type string `json:"type"`
	// Usage represents the usage of the API.
	Usage *Usage `json:"usage"`
}

// Usage represents the usage of the API.
type Usage struct {
	// InputTokens is the number of tokens used as input to the model.
	InputTokens int `json:"input_tokens"`
	// OutputTokens is the number of tokens generated by the model.
	OutputTokens int `json:"output_tokens"`
	// CacheCreationInputTokens is number of input tokens used to create the cache entry.
	CacheCreationInputTokens int `json:"cache_creation_input_tokens"`
	// CacheReadInputTokens is the number of input tokens read from the cache.
	CacheReadInputTokens int `json:"cache_read_input_tokens"`
}
